import streamlit as st
from langchain_openai import ChatOpenAI
import os
import bs4
from langchain import hub
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_upstage import ChatUpstage
from langchain_upstage import UpstageEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever

# Streamlit UI ì„¤ì •
#st.set_page_config(page_title="ì œ2ì˜ ë‚˜ë¼ chat", page_icon=":video_game:")
#st.header("ì œ2ì˜ ë‚˜ë¼ chat *^^*")
# Streamlit UI ì„¤ì •


import streamlit as st

# CSSë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì´ë“œë°” ë°°ê²½ ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ ì„¤ì •
sidebar_style = """
<style>
[data-testid="stSidebar"] {
    background-image: url("https://raw.githubusercontent.com/JIYUN0710/Langchain/9d34069783f1e72e837cee1333c95a03786732c8/crossworld_2.jpg");
    background-size: cover;
    background-position: center center;
    background-repeat: no-repeat;
}
[data-testid="stSidebar"] > div:first-child {
    background-color: rgba(0, 0, 0, 0.5);  /* ë°˜íˆ¬ëª… ê²€ì€ìƒ‰ ì˜¤ë²„ë ˆì´ */
    padding: 20px;
}
[data-testid="stSidebar"] .stTextInput label, 
[data-testid="stSidebar"] .stSelectbox label {
    color: white !important;
    font-size: 18px !important;
    font-weight: bold !important;
}
[data-testid="stSidebar"] .stTextInput input, 
[data-testid="stSidebar"] .stSelectbox select {
    color: black !important;
    background-color: rgba(255, 255, 255, 0.8) !important;
    font-size: 16px !important;
}
</style>
"""

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="ì œ2ì˜ ë‚˜ë¼ chatbot", page_icon=":video_game:")
st.title("ğŸ’Ÿ netmarble ğŸ’Ÿ")
st.header("âœ¨ ì œ2ì˜ ë‚˜ë¼: Cross Worlds âœ¨")
st.caption("ğŸ˜„ Jiyun Park ğŸ˜„")

# ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ ì ìš©
st.markdown(sidebar_style, unsafe_allow_html=True)

# ì‚¬ì´ë“œë°” ë‚´ìš©
with st.sidebar:
    upstage_api_key = st.text_input("Upstage API Key", key="chatbot_api_key", type="password")
    people = st.selectbox('ë‹¹ì‹ ì€ ì œ2ì˜ ë‚˜ë¼ User ì¸ê°€ìš”?',
                          ('YES', 'NO'),
                          index=0)
    if people == 'YES':
        email = st.text_input('ë‹¹ì‹ ì˜ ê²Œì„ ì´ë©”ì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”')
    elif people == 'NO':
        num = st.text_input('ë‹¹ì‹ ì˜ ì‚¬ì›ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”')

# API í‚¤ í™•ì¸ ë° ë©”ì‹œì§€ í‘œì‹œ
if not upstage_api_key:
    st.warning("Upstage API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()
else:
    os.environ["UPSTAGE_API_KEY"] = upstage_api_key

# ì‚¬ìš©ì ì •ë³´ í™•ì¸ ë° ë©”ì‹œì§€ í‘œì‹œ
if people == 'YES':
    if not email:
        st.warning("ë‹¹ì‹ ì˜ ê²Œì„ ì´ë©”ì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()
elif people == 'NO':
    if not num:
        st.warning("ë‹¹ì‹ ì˜ ì‚¬ì›ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()


# CSV íŒŒì¼ ê²½ë¡œ ì…ë ¥
import streamlit as st
import pandas as pd


#csv_file_path = "C:/Users/jyp/.conda/envs/langchain-cource/00_PT/total_with_images_fake.csv"
csv_file_path = "total_with_images.csv"
loader = CSVLoader(file_path=csv_file_path)
docs = loader.load()



# text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = text_splitter.split_documents(docs)

# vectorstore & retriever
vectorstore = FAISS.from_documents(splits, UpstageEmbeddings(model="solar-embedding-1-large"))
faiss_retriever = vectorstore.as_retriever()
bm25_retriever = BM25Retriever.from_documents(splits)

# EnsembleRetriever êµ¬ì„±
retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever],  # ì‚¬ìš©í•  ê²€ìƒ‰ ëª¨ë¸ì˜ ë¦¬ìŠ¤íŠ¸
    weights=[0.7, 0.3],  # ê° ê²€ìƒ‰ ëª¨ë¸ì˜ ê²°ê³¼ì— ì ìš©í•  ê°€ì¤‘ì¹˜
    search_type="mmr",  # ê²€ìƒ‰ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ì„ ì¦ì§„ì‹œí‚¤ëŠ” MMR ë°©ì‹ì„ ì‚¬ìš©
)

# prompt
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    """
    ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. 
    ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, 
    ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ `ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤` ë¼ê³  ë‹µí•˜ì„¸ìš”.
    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
    ê·¸ë¦¬ê³  ë‹¹ì‹ ì„ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì„ ë„·ë§ˆë¸”ì˜ "ì œ2ì˜ ë‚˜ë¼"ë¼ëŠ” ëª¨ë°”ì¼ ê²Œì„ì˜ ìœ ì €ì™€ ìš´ì˜ìì…ë‹ˆë‹¤. 
    Don't narrate the answer, just answer the question. Let's think step-by-step.

    #Question: 
    {question} 

    #Context: 
    {context} 

    #Answer:
    """
)


# prompt = hub.pull("teddynote/rag-prompt-korean")
# prompt

import streamlit as st
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate

llm = ChatUpstage()

#ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Use the following context to answer the user's question: {context}"),
    ("human", "Here's our conversation history:\n{history}\n\nNow, please answer this question: {question}")
])

# prompt = ChatPromptTemplate.from_messages([
#     ("system", """You are a helpful assistant for Netmarble's mobile game "The Second Country" (ì œ2ì˜ ë‚˜ë¼).
#     Your users are players and operators of this game. Always be polite, friendly, and supportive in your responses.
#     Use the following context to answer the user's question: {context}
#     If you don't know the answer or can't find it in the given context, honestly say "ì£„ì†¡í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." (I'm sorry, I couldn't find the answer to that question in the given information.)
#     Answer in Korean, but keep technical terms and names in their original form."""),
#     ("human", "Here's our conversation history:\n{history}\n\nì œ2ì˜ ë‚˜ë¼ì— ëŒ€í•´ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”: {question}")
# ])


rag_chain = (
    {
        "context": lambda x: retriever.invoke(x["question"]),
        "history": lambda x: x["history"],
        "question": lambda x: x["question"]
    }
    | prompt
    | llm
    | StrOutputParser()
)

# rag_chain = (
#     {
#         "context": lambda x: retriever.get_relevant_documents(x["question"]),
#         "history": lambda x: x["history"],
#         "question": lambda x: x["question"]
#     }
#     | prompt
#     | llm
#     | StrOutputParser()
# )

class StreamChain:
    def __init__(self, chain):
        self.chain = chain
    
    def stream(self, query, history):
        response = self.chain.stream({"question": query, "history": history})
        complete_response = ""
        for token in response:
            yield token
            complete_response += token
        return complete_response

chain = StreamChain(rag_chain)

if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content="You are a helpful assistant.")
    ]

for message in st.session_state.messages[1:]:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
    with st.chat_message(message.type):
        st.markdown(message.content)

prompt = st.chat_input("ì œ2ì˜ ë‚˜ë¼ì— ëŒ€í•´ ë¬´ì—‡ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")

if prompt:
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("human"):
        st.markdown(prompt)
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„±
    history = "\n".join([f"{'Human' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" for m in st.session_state.messages[1:-1]])
    
    with st.chat_message("ai"):
        message_placeholder = st.empty()
        full_response = ""
        for response in chain.stream(prompt, history):
            full_response += response
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append(AIMessage(content=full_response))

st.empty()
