import streamlit as st
from langchain_openai import ChatOpenAI
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain.tools.retriever import create_retriever_tool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.callbacks.base import BaseCallbackHandler

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="ì œ2ì˜ ë‚˜ë¼ chatbot", page_icon=":video_game:")
st.title("ğŸ’Ÿ netmarble ğŸ’Ÿ")
st.header("âœ¨ ì œ2ì˜ ë‚˜ë¼: Cross Worlds âœ¨")
st.caption("ğŸ˜„ Jiyun Park ğŸ˜„")

# ì‚¬ì´ë“œë°”ì— API í‚¤ ì…ë ¥ë€ ì¶”ê°€
with st.sidebar:
    upstage_api_key = st.text_input("Upstage API Key", key="chatbot_api_key", type="password")
    tavily_api_key = st.text_input("Tavily API Key", key="tavily_api_key", type="password")

# API í‚¤ í™•ì¸
if not upstage_api_key or not tavily_api_key:
    st.info("Upstage API Keyì™€ Tavily API Keyë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

os.environ["UPSTAGE_API_KEY"] = upstage_api_key
os.environ["TAVILY_API_KEY"] = tavily_api_key

# CSV íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬
@st.cache_resource
def load_data():
    csv_file_path = "total_with_images.csv"
    loader = CSVLoader(file_path=csv_file_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)
    vectorstore = FAISS.from_documents(splits, UpstageEmbeddings(model="solar-embedding-1-large"))
    return vectorstore

vectorstore = load_data()
retriever = vectorstore.as_retriever()

retriever_tool = create_retriever_tool(
    retriever,
    "solar_search",
    "Searches any questions related to Solar. Always use this tool when user query is related to Solar!",
)

tavily_tool = TavilySearchResults()

tools = [tavily_tool, retriever_tool]

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a helpful assistant for the mobile game 'ì œ2ì˜ ë‚˜ë¼' by Netmarble. 
    ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question)ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question)ì— ë‹µí•˜ì„¸ìš”.

    ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, tavilyë¥¼ í†µí•´ ì œ2ì˜ë‚˜ë¼ë¼ëŠ” í‚¤ì›Œë“œì™€ í•¨ê»˜ ê²€ìƒ‰ì„ ì‹¤ì‹œí•˜ê³ , 
    ë§Œì•½ ê²€ìƒ‰ì„ í†µí•´ ë‹µì„ ì–»ì—ˆë‹¤ë©´, ê·¸ì— ëŒ€í•œ ì¶œì²˜ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”. 
    í•˜ì§€ë§Œ ê²€ìƒ‰í•´ë„ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ 'ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.

    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
    ê·¸ë¦¬ê³  ë‹¹ì‹ ì„ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì€ ë„·ë§ˆë¸”ì˜ 'ì œ2ì˜ ë‚˜ë¼'ë¼ëŠ” ëª¨ë°”ì¼ ê²Œì„ì˜ ìœ ì €ì™€ ìš´ì˜ìì…ë‹ˆë‹¤.

    Use the following context to answer the user's question: {context}
    """),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

llm = ChatUpstage()
agent = create_tool_calling_agent(llm, tools, prompt)

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–Œ")

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content="You are a helpful assistant for the mobile game 'ì œ2ì˜ ë‚˜ë¼' by Netmarble.")
    ]

for message in st.session_state.messages[1:]:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
    with st.chat_message(message.type):
        st.markdown(message.content)

prompt = st.chat_input("ì œ2ì˜ ë‚˜ë¼ì— ëŒ€í•´ ë¬´ì—‡ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")

if prompt:
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("human"):
        st.markdown(prompt)
    
    history = "\n".join([f"{'Human' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" for m in st.session_state.messages[1:-1]])
    
    with st.chat_message("ai"):
        stream_handler = StreamHandler(st.empty())
        try:
            response = agent_executor.invoke(
                {"input": prompt},
                config={"callbacks": [stream_handler]}
            )
            st.markdown(stream_handler.text)
            st.session_state.messages.append(AIMessage(content=stream_handler.text))
        except Exception as e:
            error_message = f"An error occurred: {str(e)}"
            st.error(error_message)
            st.session_state.messages.append(AIMessage(content=error_message))

st.empty()