import streamlit as st
from langchain_openai import ChatOpenAI
import os
import bs4
from langchain import hub
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_upstage import ChatUpstage
from langchain_upstage import UpstageEmbeddings
from langchain_core.prompts import PromptTemplate

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="ì œ2ì˜ ë‚˜ë¼ chatbot", page_icon=":video_game:")
st.title("ğŸ’Ÿ netmarble ğŸ’Ÿ")
st.header("âœ¨ ì œ2ì˜ ë‚˜ë¼: Cross Worlds âœ¨")
st.caption("ğŸ˜„ Jiyun Park ğŸ˜„")


# ì‚¬ì´ë“œë°”ì— Upstage API í‚¤ ì…ë ¥ë€ ì¶”ê°€
with st.sidebar:
    upstage_api_key = st.text_input("Upstage API Key", key="chatbot_api_key", type="password")

# ì‚¬ì´ë“œë°”ì— Tavily API í‚¤ ì…ë ¥ë€ ì¶”ê°€
with st.sidebar:
    tavily_api_key = st.text_input("Tavily API Key", key="tavily_api_key", type="password")

# Upstage API í‚¤ í™•ì¸
if upstage_api_key:
    os.environ["UPSTAGE_API_KEY"] = upstage_api_key
else:
    st.info("Upstage API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# Tavily API í‚¤ í™•ì¸
if tavily_api_key:
    os.environ["TAVILY_API_KEY"] = tavily_api_key
else:
    st.info("Tavily API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()


# CSV íŒŒì¼ ê²½ë¡œ ì…ë ¥
import streamlit as st
import pandas as pd


#csv_file_path = "C:/Users/jyp/.conda/envs/langchain-cource/00_PT/total_with_images_fake.csv"
csv_file_path = "total_with_images.csv"
loader = CSVLoader(file_path=csv_file_path)
docs = loader.load()


# text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = text_splitter.split_documents(docs)

# vectorstore & retriever
vectorstore = FAISS.from_documents(splits, UpstageEmbeddings(model="solar-embedding-1-large"))
retriever = vectorstore.as_retriever()

from langchain.tools.retriever import create_retriever_tool
retriever_tool = create_retriever_tool(
    retriever,
    "solar_search",
    "Searches any questions related to Solar. Always use this tool when user query is related to Solar!",
)

from langchain_community.tools.tavily_search import TavilySearchResults
tavily_tool = TavilySearchResults()

tools = [tavily_tool,retriever_tool]

from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain import hub


# prompt
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    """
    ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. 
    ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, tavilyë¥¼ í†µí•´ ì œ2ì˜ë‚˜ë¼ë¼ëŠ” í‚¤ì›Œë“œì™€ í•¨ê»˜ ê²€ìƒ‰ì„ ì‹¤ì‹œí•˜ê³ , ë§Œì•½
    ê²€ìƒ‰ì„ í†µí•´ ë‹µì„ ì–»ì—ˆë‹¤ë©´, ê·¸ì— ëŒ€í•œ ì¶œì²˜ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”. í•˜ì§€ë§Œ ê²€ìƒ‰í•´ë„
    ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ `ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤` ë¼ê³  ë‹µí•˜ì„¸ìš”.
    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
    ê·¸ë¦¬ê³  ë‹¹ì‹ ì„ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì„ ë„·ë§ˆë¸”ì˜ "ì œ2ì˜ ë‚˜ë¼"ë¼ëŠ” ëª¨ë°”ì¼ ê²Œì„ì˜ ìœ ì €ì™€ ìš´ì˜ìì…ë‹ˆë‹¤. 
    Don't narrate the answer, just answer the question. Let's think step-by-step.

    #Question: 
    {question} 

    #Context: 
    {context} 

    #Answer:
    """
)


# prompt = hub.pull("teddynote/rag-prompt-korean")
# prompt

import streamlit as st
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a helpful assistant for the mobile game 'ì œ2ì˜ ë‚˜ë¼' by Netmarble. 
    ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question)ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question)ì— ë‹µí•˜ì„¸ìš”.

    ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, tavilyë¥¼ í†µí•´ ì œ2ì˜ë‚˜ë¼ë¼ëŠ” í‚¤ì›Œë“œì™€ í•¨ê»˜ ê²€ìƒ‰ì„ ì‹¤ì‹œí•˜ê³ , 
    ë§Œì•½ ê²€ìƒ‰ì„ í†µí•´ ë‹µì„ ì–»ì—ˆë‹¤ë©´, ê·¸ì— ëŒ€í•œ ì¶œì²˜ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”. 
    í•˜ì§€ë§Œ ê²€ìƒ‰í•´ë„ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ 'ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.

    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
    ê·¸ë¦¬ê³  ë‹¹ì‹ ì„ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì€ ë„·ë§ˆë¸”ì˜ 'ì œ2ì˜ ë‚˜ë¼'ë¼ëŠ” ëª¨ë°”ì¼ ê²Œì„ì˜ ìœ ì €ì™€ ìš´ì˜ìì…ë‹ˆë‹¤.

    Use the following context to answer the user's question: {context}
    """),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])
llm = ChatUpstage()
agent = create_tool_calling_agent(llm, tools, prompt)


agent_executor = AgentExecutor(agent=agent, tools=tools)



rag_chain = (
    {
        "context": lambda x: retriever.get_relevant_documents(x["question"]),
        "history": lambda x: x["history"],
        "question": lambda x: x["question"]
    }
    | prompt
    | llm
    | StrOutputParser()
)

from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import asyncio

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–Œ")

class StreamChain:
    def __init__(self, executor):
        self.executor = executor
    
    async def astream(self, query, history):
        stream_handler = StreamingStdOutCallbackHandler()
        response = await self.executor.arun({"input": query, "history": history}, callbacks=[stream_handler])
        return response

chain = StreamChain(agent_executor)

if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content="You are a helpful assistant for the mobile game 'ì œ2ì˜ ë‚˜ë¼' by Netmarble.")
    ]

for message in st.session_state.messages[1:]:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
    with st.chat_message(message.type):
        st.markdown(message.content)

prompt = st.chat_input("ì œ2ì˜ ë‚˜ë¼ì— ëŒ€í•´ ë¬´ì—‡ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")

if prompt:
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("human"):
        st.markdown(prompt)
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„±
    history = "\n".join([f"{'Human' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" for m in st.session_state.messages[1:-1]])
    
    with st.chat_message("ai"):
        message_placeholder = st.empty()
        stream_handler = StreamHandler(message_placeholder)
        try:
            full_response = asyncio.run(chain.astream(prompt, history))
            message_placeholder.markdown(full_response)
        except Exception as e:
            full_response = f"An error occurred: {str(e)}"
            st.error(full_response)
    
    st.session_state.messages.append(AIMessage(content=full_response))

st.empty()
