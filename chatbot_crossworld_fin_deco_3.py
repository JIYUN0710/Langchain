import streamlit as st
from langchain_openai import ChatOpenAI
import os
import bs4
from langchain import hub
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_upstage import ChatUpstage
from langchain_upstage import UpstageEmbeddings
from langchain_core.prompts import PromptTemplate

# Streamlit UI ì„¤ì •
#st.set_page_config(page_title="ì œ2ì˜ ë‚˜ë¼ chat", page_icon=":video_game:")
#st.header("ì œ2ì˜ ë‚˜ë¼ chat *^^*")
# Streamlit UI ì„¤ì •
st.set_page_config(page_title="ì œ2ì˜ ë‚˜ë¼ chatbot", page_icon=":video_game:")
st.title("ğŸ’Ÿ netmarble ğŸ’Ÿ")
st.header("âœ¨ ì œ2ì˜ ë‚˜ë¼: Cross Worlds âœ¨")
st.caption("ğŸ˜„ Jiyun Park ğŸ˜„")

with st.sidebar:
    upstage_api_key = st.text_input("Upstage API Key", key="chatbot_api_key", type="password")
if upstage_api_key:
    os.environ["UPSTAGE_API_KEY"] = upstage_api_key
else:
    st.info("Upstage API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# ì„ íƒ ë°•ìŠ¤
with st.sidebar:
    people = st.selectbox('ë‹¹ì‹ ì€ ì œ2ì˜ ë‚˜ë¼ User ì¸ê°€ìš”?',
     ('YES', 'NO'), 
    index=0)
    if people == 'YES':
        email = st.text_input('ë‹¹ì‹ ì˜ ê²Œì„ ì´ë©”ì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”')
        if len(email)==0:
            st.stop()
    elif people == 'NO':
        num = st.text_input('ë‹¹ì‹ ì˜ ì‚¬ì›ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”')
        if len(num)==0:
            st.stop()

# CSV íŒŒì¼ ê²½ë¡œ ì…ë ¥
import streamlit as st
import pandas as pd


#csv_file_path = "C:/Users/jyp/.conda/envs/langchain-cource/00_PT/total_with_images_fake.csv"
csv_file_path = "total_with_images.csv"
loader = CSVLoader(file_path=csv_file_path)
docs = loader.load()



# text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=75)
splits = text_splitter.split_documents(docs)

# vectorstore & retriever
vectorstore = FAISS.from_documents(splits, UpstageEmbeddings(model="solar-embedding-1-large"))
retriever = vectorstore.as_retriever()

# prompt
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    """
    ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. 
    ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, 
    ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ `ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤` ë¼ê³  ë‹µí•˜ì„¸ìš”.
    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
    ê·¸ë¦¬ê³  ë‹¹ì‹ ì„ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì„ ë„·ë§ˆë¸”ì˜ "ì œ2ì˜ ë‚˜ë¼"ë¼ëŠ” ëª¨ë°”ì¼ ê²Œì„ì˜ ìœ ì €ì™€ ìš´ì˜ìì…ë‹ˆë‹¤. 
    Don't narrate the answer, just answer the question. Let's think step-by-step.

    #Question: 
    {question} 

    #Context: 
    {context} 

    #Answer:
    """
)


# prompt = hub.pull("teddynote/rag-prompt-korean")
# prompt

import streamlit as st
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate

llm = ChatUpstage()

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Use the following context to answer the user's question: {context}"),
    ("human", "Here's our conversation history:\n{history}\n\nNow, please answer this question: {question}")
])
rag_chain = (
    {
        "context": lambda x: retriever.invoke(x["question"]),
        "history": lambda x: x["history"],
        "question": lambda x: x["question"]
    }
    | prompt
    | llm
    | StrOutputParser()
)

# rag_chain = (
#     {
#         "context": lambda x: retriever.get_relevant_documents(x["question"]),
#         "history": lambda x: x["history"],
#         "question": lambda x: x["question"]
#     }
#     | prompt
#     | llm
#     | StrOutputParser()
# )

class StreamChain:
    def __init__(self, chain):
        self.chain = chain
    
    def stream(self, query, history):
        response = self.chain.stream({"question": query, "history": history})
        complete_response = ""
        for token in response:
            yield token
            complete_response += token
        return complete_response

chain = StreamChain(rag_chain)

if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content="You are a helpful assistant.")
    ]

for message in st.session_state.messages[1:]:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
    with st.chat_message(message.type):
        st.markdown(message.content)

prompt = st.chat_input("ğŸ˜„ì œ2ì˜ ë‚˜ë¼ì— ëŒ€í•´ ë¬´ì—‡ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?ğŸ˜„")

if prompt:
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("human"):
        st.markdown(prompt)
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„±
    history = "\n".join([f"{'Human' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" for m in st.session_state.messages[1:-1]])
    
    with st.chat_message("ai"):
        message_placeholder = st.empty()
        full_response = ""
        for response in chain.stream(prompt, history):
            full_response += response
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append(AIMessage(content=full_response))

st.empty()